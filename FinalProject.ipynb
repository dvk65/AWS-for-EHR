{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading all the csv files stored in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and prefix/folder name\n",
    "bucket_name = 'lambdasam-bc866238ef-us-east-1'\n",
    "prefix = 'mimic-iii-clinical-database-demo-1.4/'  # Folder containing CSV files\n",
    "\n",
    "# List all CSV files in the S3 bucket under the specified prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
    "\n",
    "# Initialize an empty dataframe for merging\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Read each CSV file from S3 into a dataframe\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=csv_file)\n",
    "    df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "    \n",
    "    # Drop duplicate columns except for 'row_id'\n",
    "    duplicate_columns = [col for col in df.columns if col in merged_df.columns and col != 'row_id']\n",
    "    df = df.drop(columns=duplicate_columns)\n",
    "    \n",
    "    # Concatenate horizontally\n",
    "    merged_df = pd.concat([merged_df, df], axis=1)\n",
    "\n",
    "print(\"merged\")\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "output_csv = 'merged_output.csv'\n",
    "merged_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Upload the merged CSV back to S3\n",
    "s3.upload_file(output_csv, bucket_name, prefix + '/combined_output/merged_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a filtered DataFrame with limited columns and rows with less than 10 NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_df = pd.read_csv(\"merged_output.csv\")\n",
    "\n",
    "merged_df_selected = merged_df[[\"admittime\", \"dischtime\", \"deathtime\", \"admission_type\", \"discharge_location\", \"language\", \"ethnicity\",\n",
    "                               \"diagnosis\", \"has_chartevents_data\", \"callout_service\", \"callout_status\", \"callout_outcome\", \"acknowledge_status\",\n",
    "                               \"label\", \"description\", \"value\", \"resultstatus\", \"costcenter\", \"cpt_suffix\", \"sectionheader\", \"subsectionheader\",\n",
    "                               \"drg_type\", \"drg_code\", \"drg_severity\", \"codesuffix\", \"short_title\", \"long_title\", \"abbreviation\", \"dbsource\",\n",
    "                               \"linksto\", \"param_type\", \"fluid\", \"dilution_text\", \"dilution_comparison\", \"interpretation\", \"text\", \"gender\",\n",
    "                               \"dob\", \"dod\", \"startdate\", \"enddate\", \"drug_type\", \"drug\", \"drug_name_poe\", \"drug_name_generic\", \"formulary_drug_cd\",\n",
    "                               \"prod_strength\", \"location\"]]\n",
    "\n",
    "\n",
    "filtered_df = merged_df_selected[merged_df_selected.isna().sum(axis=1) < 10]\n",
    "filtered_df.shape\n",
    "filtered_df.to_csv(\"csv_to_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving rows from filtered DataFrame as txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def convert_csv_rows_to_text(csv_file_path, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Open the CSV file\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "\n",
    "        # Iterate over each row in the CSV file\n",
    "        for i, row in enumerate(reader):\n",
    "            # Combine the data in the row into a single string\n",
    "            row_data = \"\\n\".join([f\"{key}: {value}\" for key, value in row.items()])\n",
    "            \n",
    "            # Define the output file path\n",
    "            output_file_path = os.path.join(output_directory, f\"row_{i+1}.txt\")\n",
    "            \n",
    "            # Write the row data to a text file\n",
    "            with open(output_file_path, mode='w', encoding='utf-8') as text_file:\n",
    "                text_file.write(row_data)\n",
    "            \n",
    "            print(f\"Row {i+1} written to {output_file_path}\")\n",
    "\n",
    "csv_file_path = 'csv_to_text.csv'\n",
    "output_directory = 'output_s3'\n",
    "convert_csv_rows_to_text(csv_file_path, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading patient text files to S3 bucket for AWS Comprehend Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the S3 bucket and prefix (folder path)\n",
    "bucket_name = 'lambdasam-bc866238ef-us-east-1'\n",
    "\n",
    "directory = \"output_s3\"\n",
    "\n",
    "for f in os.listdir(directory):\n",
    "    if f.endswith(\"txt\"):\n",
    "        path = os.path.join(directory, f)\n",
    "        s3.upload_file(path, bucket_name, f'text_files/{f}')\n",
    "        print(f)\n",
    "    else:\n",
    "        print(\"not txt\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Comprehend Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "comprehend_medical = boto3.client('comprehendmedical')\n",
    "\n",
    "def process_files(input_bucket, output_bucket, input_prefix, output_prefix):\n",
    "    # List all files in the input S3 directory\n",
    "    response = s3.list_objects_v2(Bucket=input_bucket, Prefix=input_prefix)\n",
    "    \n",
    "    # Iterate over each file\n",
    "    for obj in response.get('Contents', []):\n",
    "        file_key = obj['Key']\n",
    "        \n",
    "        # Download the file\n",
    "        file_name = os.path.basename(file_key)\n",
    "        local_file_path = '/tmp/' + file_name\n",
    "        s3.download_file(input_bucket, file_key, local_file_path)\n",
    "        \n",
    "        # Read the file content\n",
    "        with open(local_file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Use Comprehend Medical to detect entities\n",
    "        result = comprehend_medical.detect_entities(Text=text)\n",
    "        entities = result['Entities']\n",
    "        \n",
    "        # Prepare the output content\n",
    "        output_content = \"\"\n",
    "        for entity in entities:\n",
    "            output_content += f\"{entity['Type']}: {entity['Text']} (Confidence: {entity['Score']})\\n\"\n",
    "        \n",
    "        # Save the output to a new file\n",
    "        output_file_name = file_name.replace('.txt', '_entities.txt')\n",
    "        output_file_path = '/tmp/' + output_file_name\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(output_content)\n",
    "        \n",
    "        # Upload the result to the output S3 directory\n",
    "        s3.upload_file(output_file_path, output_bucket, f\"{output_prefix}/{output_file_name}\")\n",
    "        print(f\"Processed {file_key} and saved entities to {output_prefix}/{output_file_name}\")\n",
    "\n",
    "# Specify your S3 bucket names and prefixes\n",
    "input_bucket = 'your-input-bucket-name'\n",
    "output_bucket = 'your-output-bucket-name'\n",
    "input_prefix = 'input-directory/'\n",
    "output_prefix = 'output-directory'\n",
    "\n",
    "# Run the processing function\n",
    "process_files(input_bucket, output_bucket, input_prefix, output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and formatting output from Comprehend Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the S3 bucket and prefix (folder path)\n",
    "bucket_name = 'lambdasam-bc866238ef-us-east-1'\n",
    "bucket_prefix = 'extracted_entities'\n",
    "directory = \"output_cm\"\n",
    "\n",
    "response = s3.list_objects_v2(Bucket = bucket_name, Prefix = bucket_prefix)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.out'):\n",
    "            # Download the .out file\n",
    "            local_file_path = os.path.join(directory, os.path.basename(key))\n",
    "            s3.download_file(bucket_name, key, local_file_path)\n",
    "\n",
    "            # Convert the .txt.json file to .txt\n",
    "            if local_file_path.endswith('.txt.out'):\n",
    "                new_file_path = local_file_path.replace('.txt.out', '.txt')\n",
    "                os.rename(local_file_path, new_file_path)\n",
    "                print(f'Converted: {local_file_path} -> {new_file_path}')\n",
    "else:\n",
    "    print(\"No .out files found in the S3 bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n",
    "\n",
    "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-bucket.html\n",
    "\n",
    "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/successfully-import-an-s3-bucket-as-an-aws-cloudformation-stack.html\n",
    "\n",
    "https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazoncloudfront.html\n",
    "\n",
    "https://docs.aws.amazon.com/service-authorization/latest/reference/list_awscloudformation.html\n",
    "\n",
    "https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazoncomprehendmedical.html\n",
    "\n",
    "https://docs.aws.amazon.com/comprehend-medical/latest/dev/comprehendmedical-gettingstarted.html\n",
    "\n",
    "https://docs.aws.amazon.com/service-authorization/latest/reference/list_awsidentityandaccessmanagementiam.html\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n",
    "\n",
    "https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonbedrock.html\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cloudformation.html\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
